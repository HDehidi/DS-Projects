{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Clustering Algorithms\n",
    "\n",
    "Unsupervised learning has many different flavors. Clustering is a large sub-field of unsupervised clustering. There are various clustering algorithms. We will cover a smaller subset. For all the methods within scikit-learn and their overview see this [link](https://scikit-learn.org/stable/modules/clustering.html).\n",
    "\n",
    "Clustering algorithms mostly support the `fit` and `predict` functions as we will see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create toy data\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples = 300, centers = 4,\n",
    "                       cluster_std = 0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], s = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y_kmeans, s = 20, cmap = \"viridis\")\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c = \"black\", s = 200, alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "X_iris = iris_data[\"data\"]\n",
    "\n",
    "# Use two dimensions to make visualization easier\n",
    "X_iris = X_iris[:, [0,3]] \n",
    "y_iris = iris_data[\"target\"]\n",
    "\n",
    "# We know that there are 3 clusters\n",
    "kmeans = KMeans(n_clusters = 3)\n",
    "kmeans.fit(X_iris)\n",
    "ykm = kmeans.predict(X_iris)\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "ax = plt.subplot(1,2,1)\n",
    "#Data\n",
    "ax.scatter(X_iris[:, 0], X_iris[:, 1], c = ykm, s = 50, cmap = \"viridis\")\n",
    "#Large cluster Center\n",
    "ax.scatter(centers[:, 0], centers[:, 1], c = \"black\", s = 200, alpha = 0.5)\n",
    "ax.set_title('K-Means Cluster')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "ax.scatter(X_iris[:, 0], X_iris[:, 1], c = y_iris, s = 50, cmap = \"viridis\")\n",
    "ax.set_title('Real Classes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the classification score (taking the cluster membership as the predicted class). We are going to use the `adjusted_rand_score` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "print('K-Means performance:',adjusted_rand_score(y_iris, ykm))\n",
    "print('Accuracy of Random Prediction:',1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Clusters**\n",
    "\n",
    "We are supplying the number of clusters. In a realistic problem, we are more likely to not know this beforehand. There are multiple ways of picking number of clusters. We will see a simple idea called silhouette analysis, most suitable for K-Means. We calculate the silhouette score of a clustering outcome and pick the best one among multiple. Depending on the randomness of the initialization, we sometimes repeat the clustering with the same number of clusters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "def calcNumClustersWithSihouette(X):\n",
    "    cluster_nums = np.arange(2,11)\n",
    "\n",
    "    models = {}\n",
    "    scores = {}\n",
    "\n",
    "    for n_c in cluster_nums:\n",
    "        model = KMeans(n_c)\n",
    "        cluster_membership = model.fit_predict(X)\n",
    "        scores[n_c] = silhouette_score(X, cluster_membership)\n",
    "        models[n_c] = model\n",
    "\n",
    "    np_scores = np.array(list(scores.values()))\n",
    "    plt.plot(cluster_nums,np_scores)\n",
    "    plt.gca().axvline(x=cluster_nums[np_scores.argmax()], color=\"k\", linestyle=\"--\")\n",
    "    print(f'Highest Silhouette Performance is with {cluster_nums[np_scores.argmax()]} number of clusters and score {np_scores.max()}')\n",
    "    \n",
    "    return models[cluster_nums[np_scores.argmax()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcNumClustersWithSihouette(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcNumClustersWithSihouette(X_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette scores are not always the best. More insight can be had by looking into silhouette plots. \n",
    "\n",
    "The below code is mainly taken from scikit-learn's [example](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Mixture Models (GMMs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib as mpl\n",
    "\n",
    "gmm = GaussianMixture(n_components = 4)\n",
    "gmm.fit(X)\n",
    "y_gmm = gmm.predict(X)\n",
    "\n",
    "y_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c = y_gmm, s = 20, cmap = \"viridis\")\n",
    "\n",
    "centers = gmm.means_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c = \"black\", s = 200, alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize the covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ellipses(gmm, ax, cmap):\n",
    "    a = plt.get_cmap(cmap)\n",
    "    colors = [a(1.*i/(gmm.n_components-1)) for i in range(gmm.n_components)]\n",
    "    for n, color in enumerate(colors):\n",
    "        if gmm.covariance_type == \"full\":\n",
    "            covariances = gmm.covariances_[n][:2, :2]\n",
    "        elif gmm.covariance_type == \"tied\":\n",
    "            covariances = gmm.covariances_[:2, :2]\n",
    "        elif gmm.covariance_type == \"diag\":\n",
    "            covariances = np.diag(gmm.covariances_[n][:2])\n",
    "        elif gmm.covariance_type == \"spherical\":\n",
    "            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n",
    "        v, w = np.linalg.eigh(covariances)\n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        angle = np.arctan2(u[1], u[0])\n",
    "        angle = 180 * angle / np.pi # convert to degrees\n",
    "        v = 3.0 * np.sqrt(2.0) * np.sqrt(v)\n",
    "        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1], 180 + angle, color = color)\n",
    "        ell.set_clip_box(ax.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "        ax.set_aspect(\"equal\", \"datalim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.gca()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y_gmm, s = 10, cmap = \"viridis\")\n",
    "make_ellipses(gmm, ax, \"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since GMM is a distribution, we can get cluster membership probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gmm_probs = gmm.predict_proba(X) #argmax of columns is used to assign cluster membership\n",
    "print(gmm_probs.shape)\n",
    "print(gmm_probs[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The iris Data set\n",
    "\n",
    "# We know that there are 3 clusters\n",
    "gmm = GaussianMixture(n_components = 3)\n",
    "gmm.fit(X_iris)\n",
    "ygmm = gmm.predict(X_iris)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "ax = plt.subplot(1,2,1)\n",
    "#Data\n",
    "ax.scatter(X_iris[:, 0], X_iris[:, 1], c = ygmm, s = 50, cmap = \"viridis\")\n",
    "make_ellipses(gmm, ax, \"viridis\")\n",
    "ax.set_title('GMM Predictions')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "ax.scatter(X_iris[:, 0], X_iris[:, 1], c = y_iris, s = 50, cmap = \"viridis\")\n",
    "ax.set_title('Real Classes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GMM performance:',adjusted_rand_score(y_iris, ygmm))\n",
    "print('Accuracy of Random Prediction:',1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agglomerative Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters = 3)\n",
    "yac = ac.fit_predict(X_iris)\n",
    "plt.scatter(X_iris[:, 0], X_iris[:, 1], c = yac, s = 50, cmap = \"viridis\")\n",
    "plt.show()\n",
    "\n",
    "print('Agglomerative Clustering Performance:',adjusted_rand_score(y_iris, yac))\n",
    "print('Accuracy of Random Prediction:',1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac2 = AgglomerativeClustering(n_clusters = 3, linkage = \"average\")\n",
    "yac2 = ac2.fit_predict(X_iris)\n",
    "plt.scatter(X_iris[:, 0], X_iris[:, 1], c = yac2, s = 50, cmap = \"viridis\")\n",
    "plt.show()\n",
    "\n",
    "print('Agglomerative Clustering Performance with Average Linkage:',adjusted_rand_score(y_iris, yac2))\n",
    "print('Accuracy of Random Prediction:',1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spectral Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "sc = SpectralClustering(3, n_init=100, assign_labels='discretize')\n",
    "y_sc  = sc.fit_predict(X_iris)  \n",
    "\n",
    "plt.scatter(X_iris[:, 0], X_iris[:, 1], c = y_sc, s = 50, cmap = \"viridis\")\n",
    "plt.show()\n",
    "\n",
    "print('Agglomerative Clustering Performance with Average Linkage:',adjusted_rand_score(y_iris, y_sc))\n",
    "print('Accuracy of Random Prediction:',1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Digits**\n",
    "\n",
    "Let's try these on the digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits[\"data\"]\n",
    "y = digits[\"target\"]\n",
    "Xtsne = TSNE(n_components = 2).fit_transform(X)\n",
    "km = KMeans(n_clusters = 10)\n",
    "ykmeans = km.fit_predict(Xtsne)\n",
    "\n",
    "print('TSNE + KMeans Performance:',adjusted_rand_score(y, ykmeans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotDigitProjections(Xin, y, title = None, show = True):\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    for c in range(10):\n",
    "        indexes = np.where(y == c)[0]\n",
    "        x = Xin[indexes]\n",
    "        plt.scatter(x[:, 0], x[:, 1], label = c)\n",
    "    plt.xlabel(\"component 1\")\n",
    "    plt.ylabel(\"component 2\")\n",
    "    plt.legend()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if(show):\n",
    "        plt.show()\n",
    "\n",
    "plotDigitProjections(Xtsne, ykmeans, \"TSNE - KMeans Labels\")\n",
    "plotDigitProjections(Xtsne, y, \"TSNE - True Labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about PCA? Also to show that we can use unsupervised method in pipelines\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([(\"pca\", PCA(n_components = 2)),\n",
    "                 (\"kmeans\", KMeans(n_clusters = 10))])\n",
    "\n",
    "pipe.fit(X)\n",
    "y_pca_km = pipe.predict(X)\n",
    "X_pca = pipe.named_steps[\"pca\"].transform(X)\n",
    "\n",
    "print('PCA + KMeans Performance:',adjusted_rand_score(y, y_pca_km))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDigitProjections(X_pca, y_pca_km, \"PCA - KMeans\", True)\n",
    "plotDigitProjections(X_pca, y, \"PCA - True Labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"pca\", PCA(n_components = 40)),\n",
    "                 (\"kmeans\", KMeans(n_clusters = 10))])\n",
    "\n",
    "pipe.fit(X)\n",
    "y_pca_km = pipe.predict(X)\n",
    "X_pca = pipe.named_steps[\"pca\"].transform(X)\n",
    "\n",
    "print('PCA (40) + KMeans Performance:',adjusted_rand_score(y, y_pca_km))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about no dimensionality reduction? - KMeans\n",
    "\n",
    "kmeans_full = KMeans(n_clusters=10)\n",
    "y_full = kmeans_full.fit_predict(X)\n",
    "\n",
    "print('Full KMeans Performance:',adjusted_rand_score(y, y_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about no dimensionality reduction? - GMM\n",
    "\n",
    "kmeans_full = GaussianMixture(n_components=10)\n",
    "y_full = kmeans_full.fit_predict(X)\n",
    "\n",
    "print('Full GMM Performance:',adjusted_rand_score(y, y_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "pipe_umap = Pipeline([(\"umap\", UMAP(n_components = 2)),\n",
    "                      (\"kmeans\", KMeans(n_clusters = 10))])\n",
    "\n",
    "pipe_umap.fit(X)\n",
    "y_umap_km = pipe_umap.predict(X)\n",
    "X_umap = pipe_umap.named_steps[\"umap\"].transform(X)\n",
    "\n",
    "print('UMAP + KMeans Performance:',adjusted_rand_score(y, y_umap_km))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDigitProjections(X_umap, y_umap_km, \"UMAP - KMeans\", True)\n",
    "plotDigitProjections(X_umap, y, \"UMAP - True Labels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
